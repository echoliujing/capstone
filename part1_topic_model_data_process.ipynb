{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b6970e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import html\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LdaModel as gensim_LdaModel, CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# libs for saving model\n",
    "import pickle\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8216d041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714822f",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "### Define the data structure of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c5f440ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_gold - true/false - If true, will only match if the author has reddit gold. If false, will only match if they do not have gold.\n",
    "# is_submitter - true/false - (only relevant when checking comments) If true, will only match if the author \n",
    "# was also the submitter of the post being commented inside. If false, will only match if they were not.\n",
    "# send_replies â€“ When True, messages will be sent to the submission author when comments are made to the submission\n",
    "\n",
    "data_type = {\"subreddit\": \"string\", \"subreddit_id\": \"string\", \"subreddit_type\": \"string\", \"author\": \"string\", \"body\" : \"string\", \n",
    "            \"created_date\" : \"string\", \"created_utc\": \"string\", \"retrieved_on\" : \"string\", \n",
    "            \"id\": \"string\", \"parent_id\": \"string\", \"link_id\": \"string\", \"score\": \"int\", \"total_awards_received\": \"int\", \n",
    "            \"controversiality\": \"int\", \"gilded\": \"int\", \n",
    "            \"collapsed_because_crowd_control\": \"int\", \"collapsed_reason\": \"string\", \"distinguished\": \"string\", \"removal_reason\": \"string\",\n",
    "            \"author_created_utc\": \"string\", \"author_fullname\": \"string\", \"author_patreon_flair\": \"bool\", \"author_premium\": \"bool\",\n",
    "            \"can_gild\": \"bool\", \"can_mod_post\": \"bool\", \"collapsed\": \"bool\", \"is_submitter\": \"bool\", \"_edited\": \"string\", \"locked\": \"bool\",\n",
    "            \"quarantined\": \"bool\", \"no_follow\": \"bool\", \"send_replies\": \"bool\", \"stickied\": \"bool\", \"author_flair_text\": \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f883420",
   "metadata": {},
   "source": [
    "### read all the csv files using the defined data structure, remove deleted rows\n",
    "\n",
    "Some of the rows are marked as \"deleted\" which indicate the contents had been removed for some reasons. We need to remove then as they do not contribute to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7dd60c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x in range(2008, 2020):\\n    df = pd.read_csv(f'./data/{x}.csv')\\n    df = df[(df.body != '[deleted]') & (df.body != '[removed]')]\\n    df.drop(columns = ['Unnamed: 0'], inplace = True)\\n    df.to_csv(f'./data/{x}_revised.csv')\\n\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data and remove [deleted] row\n",
    "# we comment out the codes as we already process the data\n",
    "\n",
    "\"\"\"\n",
    "for x in range(2008, 2020):\n",
    "    df = pd.read_csv(f'./data/{x}.csv')\n",
    "    df = df[(df.body != '[deleted]') & (df.body != '[removed]')]\n",
    "    df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    df.to_csv(f'./data/{x}_revised.csv')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc409f27",
   "metadata": {},
   "source": [
    "### Identify All Posts Related to iPhone on Reddit\n",
    "\n",
    "We conducted a review and found that the subreddits **apple** and **iphone** contain the most relevant discussions about the iPhone. However, the 'apple' subreddit also includes posts about other Apple products like Macs, iPod Touch, and iTunes. To ensure relevance, we will filter out these non-iPhone related posts. Our approach assumes that all posts within a single discussion thread focus on the same topic. Therefore, we will retain any thread in the 'apple' subreddit if at least one post within that thread mentions the iPhone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e0cbee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_iphone_post(data):\n",
    "    \"\"\"\n",
    "    Find the parent_ids that contain discussions about iPhone.\n",
    "    \"\"\"\n",
    "    df_apple = data[data.subreddit == 'apple']\n",
    "    related_discussion = df_apple[df_apple.body.str.contains('iphone', flags = re.IGNORECASE)]\n",
    "    return(related_discussion.link_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ac180e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_iphone_post(data):\n",
    "    ids = locate_iphone_post(data)\n",
    "    return(data[(data.subreddit == 'iphone') | (data.link_id.isin(ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bfe82ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final clean, only retain iPhone related posts each year\n",
    "\n",
    "def get_iphone_data_yearly(data, filename = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = extract_all_iphone_post(data)\n",
    "    df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    if filename != None:\n",
    "        df.to_csv(filename)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "7fd7ef2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x in range(2008, 2020):\\n    file_name = f'./data/{x}_revised.csv'\\n    save_path = f'./data/{x}_iphone_v2.csv'\\n    data = pd.read_csv(file_name, dtype=data_type, header = 0)\\n    get_iphone_data_yearly(data, filename = save_path)\\n\""
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment the following code because we already run it\n",
    "\n",
    "\"\"\"\n",
    "for x in range(2008, 2020):\n",
    "    file_name = f'./data/{x}_revised.csv'\n",
    "    save_path = f'./data/{x}_iphone_v2.csv'\n",
    "    data = pd.read_csv(file_name, dtype=data_type, header = 0)\n",
    "    get_iphone_data_yearly(data, filename = save_path)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d84e32",
   "metadata": {},
   "source": [
    "### process the special chars in the comment\n",
    "\n",
    "It seems that the Reddit comment data are raw text data which means the HTML character entities are preserved. For example, we ofter encounter the following chars in the comment:\n",
    "\n",
    "1. `&lt;` is an HTML entity for the less-than symbol (\"<\").\n",
    "2. `&gt;` is an HTML entity for the greater-than symbol (\">\").\n",
    "\n",
    "We need to convert these entities back to their original characters to ensure that the text is correctly interpreted and analyzed. This can usually be done using HTML parsing libraries.\n",
    "\n",
    "Also, escape sequences such as `\\n` and `\\'` need to be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8f5184fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape_seq(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    while data.body.str.contains('&gt;').sum() > 0:\n",
    "        data['body'] = data['body'].apply(html.unescape)\n",
    "        \n",
    "    # Replace common escape sequences\n",
    "    escape_sequences = {\"\\\\n\": \" \", \"\\\\r\": \" \", \"\\\\t\": \" \", \"\\\\'\": \"'\", '\\\\\"': '\"', '\\\\b': '', '\\\\0': ''}\n",
    "    for seq, replacement in escape_sequences.items():\n",
    "        data['body'] = data['body'].str.replace(seq, replacement, regex=False)\n",
    "    \n",
    "    data['body'] = data.body.apply(lambda x: x.strip())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "56ece07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_create_utc(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Ensure 'created_utc' is a datetime object\n",
    "    data['created_utc'] = pd.to_datetime(data['created_utc'])\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "48fc882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_post_by_link_id(data) -> pd.Series:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Group by 'link_id', sort by 'created_utc', and concatenate 'body'\n",
    "    grouped_texts = data.sort_values(by='created_utc').groupby('link_id')['body'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "    return grouped_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "564c9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing function, or reduce noises\n",
    "def combine_special_words(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # mark special words\n",
    "    \"\"\"\n",
    "    sw = {'face id': 'face-id', 'touch id': 'touch-id', 'apple pay': 'apple-pay', '3d touch': '3d-touch',\n",
    "         'ipad pro': 'ipad-pro', 'ipod pro': 'ipod-pro', 'ipadpro': 'ipad-pro', 'ipodprod': 'ipod-pro',\n",
    "         'mac pro': 'mac-pro', 'macpro': 'mac-pro', 'apple watch': 'apple-watch', 'applewatch': 'apple-watch',\n",
    "         'homepod': 'home-pod', 'home pod': 'home-pod', 'apple tv': 'apple-tv', 'appletv': 'apple-tv', 'apple music': 'apple-music',\n",
    "         'applemusic': 'apple-music', 'lightning cable': 'lightning-cable', 'true tone': 'true-tone', \n",
    "         'retina display': 'retina-display',  'dark mode': 'dark-mode', 'rose gold': 'rose-gold', 'jet black': 'jet black', \n",
    "         'space gray': 'space-gray', 'oled display': 'oled-display', 'truedepth camera': 'truedepth-camera', \n",
    "         'truedepth': 'truedepth-camera', 'night shift': 'night-shift', 'portrait mode': 'portrait-mode', 'live photos': 'live-photos', \n",
    "         'live photo': 'live-photos', 'force touch': 'force-touch', 'slo-mo video': 'slo-mo-video', 'battery life': 'battery-life',\n",
    "         'wireless charger': 'wireless-charger', 'colour': 'color', 'google map': 'google-map', 'macbook pro': 'mac-pro',\n",
    "         'macbook air': 'mac-air', 'ipad air': 'ipad-air', 'release date': 'release-date', 'wi fi': 'wifi', '30 pin': '30-pin', \n",
    "         'black friday': 'black-friday', 'ipad mini': 'ipad-mini', 'ipod touch': 'ipod-touch'}\n",
    "    \n",
    "    sw2 = {'lightning': 'lightning-cable', 'retina': 'retina-display', 'oled': 'oled-display', 'portrait': 'portrait-mode',\n",
    "          'jailbreaking': 'jailbreak', 'jailbroken': 'jailbreak'}\n",
    "          \n",
    "    for seq, replacement in sw.items():\n",
    "        data['body'] = data['body'].str.replace(seq, replacement, regex=False)\n",
    "    data['body'] = data.body.apply(lambda x: x.strip())\n",
    "    \n",
    "    for seq, replacement in sw2.items():\n",
    "        data['body'] = data['body'].str.replace(seq, replacement, regex=False)\n",
    "    data['body'] = data.body.apply(lambda x: x.strip())\n",
    "    \"\"\"\n",
    "    \n",
    "    good_synonyms = {\n",
    "    \"nice\": \"good\",\n",
    "    \"excellent\": \"good\",\n",
    "    \"great\": \"good\",\n",
    "    \"amazing\": \"good\",\n",
    "    \"fantastic\": \"good\",\n",
    "    \"awesome\": \"good\",\n",
    "    \"wonderful\": \"good\",\n",
    "    \"perfect\": \"good\",\n",
    "    \"fabulous\": \"good\",\n",
    "    \"superb\": \"good\",\n",
    "    \"outstanding\": \"good\",\n",
    "    \"beautiful\": \"good\",\n",
    "    \"remarkable\": \"good\",\n",
    "    \"impressive\": \"good\",\n",
    "    \"splendid\": \"good\",\n",
    "    \"terrific\": \"good\",\n",
    "    \"marvelous\": \"good\",\n",
    "    \"positive\": \"good\",\n",
    "    \"satisfactory\": \"good\",\n",
    "    \"pretty\": \"good\",\n",
    "    \"pleasing\": \"good\"}\n",
    "    \n",
    "    bad_synonyms = {\n",
    "    \"poor\": \"bad\",\n",
    "    \"terrible\": \"bad\",\n",
    "    \"awful\": \"bad\",\n",
    "    \"horrible\": \"bad\",\n",
    "    \"dreadful\": \"bad\",\n",
    "    \"abysmal\": \"bad\",\n",
    "    \"worse\": \"bad\",\n",
    "    \"lousy\": \"bad\",\n",
    "    \"atrocious\": \"bad\",\n",
    "    \"inferior\": \"bad\",\n",
    "    \"unsatisfactory\": \"bad\",\n",
    "    \"inadequate\": \"bad\",\n",
    "    \"substandard\": \"bad\",\n",
    "    \"unsuitable\": \"bad\",\n",
    "    \"unpleasant\": \"bad\",\n",
    "    \"negative\": \"bad\",\n",
    "    \"deficient\": \"bad\",\n",
    "    \"mediocre\": \"bad\",\n",
    "    \"pathetic\": \"bad\",\n",
    "    \"lacking\": \"bad\",\n",
    "    \"undesirable\": \"bad\"}\n",
    "    \n",
    "    for seq, replacement in good_synonyms.items():\n",
    "        data['body'] = data['body'].str.replace(seq, replacement, regex=False)\n",
    "    data['body'] = data.body.apply(lambda x: x.strip())\n",
    "    \n",
    "    for seq, replacement in bad_synonyms.items():\n",
    "        data['body'] = data['body'].str.replace(seq, replacement, regex=False)\n",
    "    data['body'] = data.body.apply(lambda x: x.strip())\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "92f14581",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "improved_sw_list = ['gonna', 'understand', 'seen', 'wanted', 'haha', 'max', 'restart', 'deleted', 'happening', 'possible', 'understand', \n",
    "                    'literally', 'matter', 'based', 'regarding', 'possible', 'latest', 'np', 'handed', 'iphone', 'use', 'people', 'http', \n",
    "                    'https', 'www', 'com', 'really', 'apple', 'actually', 'thanks', 'thank', 'think', 'phone', 'oh', 'sorry', 'hi', 'imgur', \n",
    "                    'like', 'get', 'got', 'used', 'make', 'work', 'worked', 'apps', 'want' 'wants', 'file', 'thing', 'say', 'know', \n",
    "                    'knew', 'reddit', 'subreddit', 'need', 'using', 'app', 'year', 'month', 'day', 'going', 'window', 'product', 'good', \n",
    "                    'pay', 'song', 'way', 'love', 'great', 'free', 'point', 'product', 'new', 'try', 'fix', 'work', 'issue', 'problem', 'got', \n",
    "                    'store', 'order', 'ordered', 'date', 'ship', 'shipping', 'org', 'usage', 'wikipedia', 'updates', 'update', 'feature', \n",
    "                    'gb', 'backup', 'file', 'space', 'lot', 'time', 'better', 'look', 'right', 'maybe', 'might', 'can', 'could', 'be', 'come', \n",
    "                    'device', 'user', 'run', 'nice', 'version', 'buy', 'software', 'hardware', 'application', 'mac', 'article', 'commit', \n",
    "                    'comment', 'opinion', 'reason', 'mean', 'little', 'computer', 'job', 'mobile', 'market', 'sure', 'yes', 'no', 'let', \n",
    "                    'probably', 'game', 'price', 'jailbreaking', 'open', 'close', 'best', 'fun', 'guy', 'gay', 'shit', 'past', 'fit', \n",
    "                    'pretty', 'cool', 'long', 'story', 'real', 'video', 'company', 'platform', 'version', 'case', 'getting', 'idea', \n",
    "                    'bought', 'play', 'post', 'link', 'review', 'awesome', 'said', 'fact', 'different', 'making', 'technology', 'service', \n",
    "                    'data', 'icon', 'home', 'page', 'button', 'jpg', 'png', 'imac', 'pc', 'desktop', 'drive', 'music', 'developer', 'dev', \n",
    "                    '99', 'message', 'text', 'hour', 'bit', 'unlocked', 'customer', 'io', 'help', 'release', 'able', 'took', 'feel', 'felt', \n",
    "                    'turn', 'street', 'city', 'town', 'cheng', 'check', 'delete', 'add', 'enter', 'quality', 'sound', 'support', 'team', \n",
    "                    'read', 'ad', 'iphones', 'old', 'new', 'tv', 'control', 'plan', 'business', 'hand', 'big', 'download', 'upload', \n",
    "                    'complain', 'install', 'installed', 'wait', 'code', 'site', 'edit', 'option', 'image', 'year', 'thought', 'word', \n",
    "                    'trying', 'try', 'tell', 'paid', 'install', 'friend', 'question', 'man', 'woman', 'girl', 'boy', 'kind', 'hope', \n",
    "                    'hopefully', 'tried', 'account', 'number', 'called', 'laptop', 'called', 'week', 'launch', 'password', 'money', 'dollar', \n",
    "                    'lol', 'wow', 'machine', 'gen', 'paying', 'buying', 'op', 'definitely', 'second', 'running', 'worked', 'agree', \n",
    "                    'wrong', 'place', 'forum', 'far', 'away', 'click', 'talk', 'car', 'came', 'difference', 'hard', 'reading', 'hold', \n",
    "                    'example', 'notification', 'happens', 'listen', 'see', 'copy', 'access', 'looking', 'default', 'miss', 'switch', \n",
    "                    'switched', 'happened', 'start', 'started', 'note', 'today', 'wallpaper', 'change', 'close', 'today', 'wallpaper', \n",
    "                    'change', 'close', 'likely', 'mail', 'email', 'list', 'push', 'reset', 'went', 'charging', 'ask', 'answer', \n",
    "                    'ago', 'asked', 'guess', 'useful', 'easy', 'playing', 'sell', 'cost', 'black', 'white', 'setting', 'report', 'told', \n",
    "                    'tomorrow', 'morning', 'hey', 'night', 'simple', 'step', 'night', 'simple', 'step', 'picture', '10', '100', '200',\n",
    "                    'shipped', '14', '13', '100', '50', 'fucking', 'party', 'mt', '30', 'screen', 'test', 'remote', 'fine', '9am', \n",
    "                    'et', '9am et', 'automatically', 'posted', 'posted', 'regrettably', 'posted', 'noticed', 'tablet', 'model', \n",
    "                    'calling', 'wi fi', 'send', '2fapple', '2fr', 'rule', 'available', 'available', 'keyboard', 'keyboard', \n",
    "                    'thread' ,'inbox', 'mailbox', 'performed', 'alarm', 'le', 'stuff', 'subreddit compose', 'moderator subreddit compose', \n",
    "                    'compose', 'normal', 'end', 'topic', 'saturday', 'wednesday', 'self', 'promotion', 'bank', 'sr', 'sort', '100', \n",
    "                    '100', '20', 'pro', 'air', 'ipads', 'id', 'retina', 'display', 'lightning', 'mode', 'downloading', 'unlimited', 'mini', \n",
    "                    'saying', 'save', 'sm', 'yeah', 'want', 'imgur', 'www', \n",
    "                    'turned', 'on', 'later', 'soon', 'express', 'reposting', 'iup', 'left', 'type', 'instead', 'luck', 'hello', 'congrats',\n",
    "                    'ok', 'winning', 'win', '20the', '20this', '20of', '20by', '2fu', 'fall', '20have', 'notice', 'cheaper', 'current', 'value',\n",
    "                    'random', 'att', '19', 'minute', 'yesterday', 'color', 'set', 'easier', '24', '25', 'ebay', 'older', 'line', \n",
    "                   ]\n",
    "\n",
    "stopwords.extend(improved_sw_list)\n",
    "stop_words_list = set(stopwords)\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing non-alphabetic characters\n",
    "    # text = re.sub(r'-', '_', text)\n",
    "    text = re.sub(r'[^a-z0-9&-]', ' ', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Removing stopwords and lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words_list]\n",
    "    # Rejoining the words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "02d49cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns_adjectives(text):\n",
    "    # Tokenize the text\n",
    "    text = re.sub(r'[^a-zA-Z0-9&-]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # POS Tagging\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Filter nouns and adjectives\n",
    "    nouns_adj = [word for word, tag in tagged if tag.startswith('NN') or tag.startswith('JJ')]\n",
    "\n",
    "    # Lemmatization and stopword removal\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in nouns_adj if word.lower() not in stop_words_list]\n",
    "    filtered_words = ' '.join(filtered_words)\n",
    "    filtered_words.lower()\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b6a09462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_data_process(posts: pd.Series) -> pd.Series:\n",
    "    cleaned_posts = posts.apply(clean_text)\n",
    "    #cleaned_posts = posts.apply(extract_nouns_adjectives)\n",
    "    return(cleaned_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0623f3c",
   "metadata": {},
   "source": [
    "LDA\n",
    "\n",
    "### Methodology ###\n",
    "1. Data Collection and Preparation:\n",
    "\n",
    "   * We collected a substantial dataset of Reddit comments, year-wise, to ensure a comprehensive analysis.\n",
    "   * Rigorous preprocessing was performed on this textual data to clean and standardize it for effective topic modeling.\n",
    "  \n",
    "  \n",
    "2. Topic Extraction Using LDA:\n",
    "\n",
    "   * For each year's data, we applied Latent Dirichlet Allocation (LDA), a powerful technique for topic modeling.\n",
    "   * To determine the optimal number of topics for each year, we utilized GridSearch provided by scikit-learn. This approach allowed us to identify the most coherent and meaningful number of topics for each year's dataset.\n",
    "   \n",
    "   \n",
    "3. Observations:\n",
    "\n",
    "   * A notable trend emerged from our analysis: the number of topics increased with each passing year.\n",
    "   * This increase is attributed to two primary factors:\n",
    "      * Growing Data Volume: As the volume of Reddit comments expanded annually, it naturally led to a broader spectrum of discussions.\n",
    "      * Divergence of Topics: Over the years, the discussions on Reddit became more diverse and multifaceted, reflecting the evolving interests and concerns of the Reddit community.\n",
    "      \n",
    "      \n",
    "4. Manual Review and Categorization:\n",
    "\n",
    "   * With the topics for each year extracted, the next step involved a manual review. This review process was crucial to deeply understand the context and nuances of each topic.\n",
    "   * Our goal is to classify these topics into a fixed number of overarching categories. This categorization will not only streamline the topics for easier comprehension but also help in identifying common or persistent themes across different years.\n",
    "   \n",
    "   \n",
    "5. Significance and Next Steps\n",
    "\n",
    "\n",
    "   * Understanding Community Evolution: This analysis provides valuable insights into how online communities evolve, highlighting changes in interests, concerns, and popular discussions over time.\n",
    "   * Strategic Application: The findings from this study can inform content strategies, marketing approaches, and community engagement plans for entities interested in leveraging Reddit's vast user base.\n",
    "   * Future Exploration: Building upon this research, we aim to explore correlations between these topics and external factors such as global events, technological advancements, and cultural shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1e170a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(posts, vectorizer = None, doc_topic_matrix = None,  num_topics = 12, num_iter = 10, tf_idf=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if (vectorizer == None) or (doc_topic_matrix == None):\n",
    "        print('--- starting processing the data in sklearn ---')\n",
    "        clean_text = nlp_data_process(posts)\n",
    "    \n",
    "        print('training WordCountVec and doc-topic-matrix')\n",
    "        # Extracting features for LDA\n",
    "        if tf_idf == True:\n",
    "            vectorizer = TfidfVectorizer(max_df = 0.8, min_df=0.02, stop_words='english', ngram_range=(1,3), \n",
    "                                     token_pattern=r'(?u)\\b\\w[\\w&-]*\\w\\b')\n",
    "        else:\n",
    "            vectorizer = CountVectorizer(max_df = 0.8, min_df = 0.02, stop_words = 'english', ngram_range = (1,3), \n",
    "                                         token_pattern=r'(?u)\\b\\w[\\w&-]*\\w\\b')\n",
    "        dtm = vectorizer.fit_transform(clean_text) # doc-word-matrix\n",
    "    else:\n",
    "        print(f'word-vec and doc-topic-matrix are provided, skip the fisrt part')\n",
    "    \n",
    "    print('training LDA model')\n",
    "    # Fitting LDA model\n",
    "    lda = LatentDirichletAllocation(n_components = num_topics, random_state = 0, n_jobs = -1, learning_decay = 0.8, max_iter = num_iter)\n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    print('--- finish training ---')\n",
    "    return((vectorizer, dtm, lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b4267948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words, save_path = ''):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d:\" % (topic_idx)] = \", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        \n",
    "    if save_path != '':\n",
    "        with open(save_path, 'w') as file:\n",
    "            for topic, words in topic_dict.items():\n",
    "                file.write(f\"{topic} {words}\\n\")\n",
    "        \n",
    "    return topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bd62e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lda_vis(vectorizer, dtm, model):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pyLDAvis.enable_notebook()\n",
    "    panel = pyLDAvis.lda_model.prepare(lda, dtm, vectorizer, mds='tsne')\n",
    "    panel\n",
    "    return(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "39767bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_topic_matrix_gensim(lda_model, corpus):\n",
    "    # Number of topics\n",
    "    num_topics = lda_model.num_topics\n",
    "\n",
    "    # Create a matrix to hold topic distributions for each document\n",
    "    doc_topic_matrix = np.zeros((len(corpus), num_topics))\n",
    "\n",
    "    # Iterate over the corpus to get topic distribution for each document\n",
    "    for i, row in enumerate(corpus):\n",
    "        # Get the distribution for the document\n",
    "        doc_topics = lda_model.get_document_topics(row, minimum_probability=0)\n",
    "        # Update the matrix\n",
    "        doc_topic_matrix[i, :] = [prob for _, prob in doc_topics]\n",
    "\n",
    "    return doc_topic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "bffb32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model_gensim(posts, num_topic = 12):\n",
    "    \"\"\"\n",
    "    Trains an LDA model using Gensim with preprocessed posts.\n",
    "    \"\"\"\n",
    "    print('--- starting processing the data ---')\n",
    "\n",
    "    # Convert preprocessed posts to list of words\n",
    "    data_words = [post.split() for post in posts]\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    \n",
    "    # Filter out tokens that appear in\n",
    "    # less than 30 documents (absolute number) or\n",
    "    # more than 80% documents (fraction of total corpus size, not absolute number).\n",
    "    # after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "    id2word.filter_extremes(no_below=30, no_above=0.8, keep_n=10000)\n",
    "\n",
    "    # Create Corpus: Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "\n",
    "    print('training LDA model')\n",
    "    lda_model = gensim_LdaModel(corpus=corpus,\n",
    "                         id2word=id2word,\n",
    "                         num_topics=num_topic,\n",
    "                         random_state=0,\n",
    "                         update_every=1,\n",
    "                         chunksize=100,\n",
    "                         passes=10,\n",
    "                         alpha='auto')\n",
    "\n",
    "    print('--- finish training ---')\n",
    "    return((id2word, corpus, lda_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "86369451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute coherence scores for various number of topics.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max number of topics\n",
    "    start : Min number of topics\n",
    "    step : Step size\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim_LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "573bcab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_display_topics(lda_model, num_top_words):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display the top words for each topic\n",
    "    for i, topic in lda_model.show_topics(formatted=True, num_topics=lda_model.num_topics, num_words=num_top_words):\n",
    "        print(f\"Topic {i}: {topic}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e0a78ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not used as we use sklearn instead of gensim\n",
    "def search_best_num_topics(posts, start = 1, stop = 40, step = 1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print('getting the id2word and corpus')\n",
    "    # Convert preprocessed posts to list of words\n",
    "    data_words = [post.split() for post in posts]\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    \n",
    "    # Filter out tokens that appear in\n",
    "    # less than 30 documents (absolute number) or\n",
    "    # more than 80% documents (fraction of total corpus size, not absolute number).\n",
    "    # after the above two steps, keep only the first 100000 most frequent tokens.\n",
    "    id2word.filter_extremes(no_below=0.02, no_above=0.8, keep_n=10000)\n",
    "    \n",
    "    # Create Corpus: Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "    \n",
    "    print('calculate the LDA model and corresponding coherence score')\n",
    "    # Assuming 'id2word' is your Gensim dictionary, 'corpus' is your Gensim corpus, and 'data_words' is your preprocessed text data\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words, start=start, limit=limit, step=step)\n",
    "    \n",
    "    return(model_list, coherence_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "93e63025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_best_esti(year, start_num, end_num, step, iters: list):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    data_path = './data/' + str(year) + '_iphone.csv'\n",
    "    df = pd.read_csv(data_path, dtype=data_type, header = 0)\n",
    "    df = remove_escape_seq(df)\n",
    "    df = convert_create_utc(df)\n",
    "    grouped_posts = aggregate_post_by_link_id(df)\n",
    "    posts = nlp_data_process(grouped_posts.body)\n",
    "    \n",
    "    vectorizer, dtm, lda = train_lda_model(posts, 6)\n",
    "\n",
    "\n",
    "    # Define Search Param\n",
    "    search_params = {'n_components': [i for i in range(start_num, end_num, step)], 'max_iter': iters}\n",
    "\n",
    "    # Init the Model\n",
    "    lda_model = LatentDirichletAllocation(n_jobs=-1)\n",
    "\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda_model, param_grid=search_params, verbose=4)\n",
    "\n",
    "    # Do the Grid Search\n",
    "    model.fit(dtm)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3ab13d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_of_year(year: int, save_path = '', save = False, tf_idf = False, version = 'v1'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # load the data\n",
    "    path_data = f'./data/{str(year)}_iphone_{version}.csv'\n",
    "    \n",
    "    # process data\n",
    "    print(f'processing the data of year {str(year)}')\n",
    "    df = pd.read_csv(path_data, dtype=data_type, header = 0)\n",
    "    df = remove_escape_seq(df)\n",
    "    df = combine_special_words(df)\n",
    "    df = convert_create_utc(df)\n",
    "    grouped_posts = aggregate_post_by_link_id(df)\n",
    "    df_posts = grouped_posts[['link_id', 'body']]\n",
    "    df_posts.to_csv(f'./data/{year}_aggregated_posts_{version}.csv')\n",
    "    posts = nlp_data_process(grouped_posts.body)\n",
    "    \n",
    "    # determine the best params, this part is done by function search_for_best_esti\n",
    "    # 2009 (2008) topic = 2, max_iter = 20\n",
    "    # 2011 topic = 4, max_iter = 40\n",
    "    # 2013 topic = 6, max_iter = 40 (2012)\n",
    "    # 2015 tioic = 12, max_iter = 60 (2014)\n",
    "    # 2016 topic = 18, n_components = 80\n",
    "    # 2017 (2018, 2019) topic = 24, n_component = 90\n",
    "    # but I find out that the result is not goood, so I have to increase the num_topic a bit\n",
    "    \n",
    "    if year == 2008:\n",
    "        num_topics = 1\n",
    "        num_iter = 10\n",
    "    elif year == 2009 or year == 2010:\n",
    "        num_topics = 9\n",
    "        num_iter = 20\n",
    "    elif year == 2011:\n",
    "        num_topics = 10\n",
    "        num_iter = 40\n",
    "    elif year == 2012:\n",
    "        # word 20\n",
    "        num_topics = 12\n",
    "        num_iter = 40\n",
    "    elif year == 2013:\n",
    "        num_topics = 12\n",
    "        num_iter = 40\n",
    "    elif year == 2014:\n",
    "        num_topics = 17\n",
    "        num_iter = 60\n",
    "    elif year == 2015:\n",
    "        num_topics = 16\n",
    "        num_iter = 60\n",
    "    elif year == 2016:\n",
    "        num_topics = 20\n",
    "        num_iter = 80\n",
    "    elif year == 2017 or year == 2018:\n",
    "        num_topics = 24\n",
    "        num_iter = 90\n",
    "    else:\n",
    "        num_topics = 21\n",
    "        num_iter = 80\n",
    "    \n",
    "    print(f'start training the model')\n",
    "    (vectorizer, dtm, lda) = train_lda_model(posts, num_topics=num_topics, num_iter=num_iter, tf_idf=tf_idf)\n",
    "    \n",
    "    # Generate the doc-topic matrix\n",
    "    # Document topic distribution for X\n",
    "    doc_topic_matrix = lda.transform(dtm)\n",
    "    \n",
    "    print(f'printing the results')\n",
    "    display_topics(lda, vectorizer.get_feature_names_out(), 30, save_path = save_path + '_topics.txt')\n",
    "    \n",
    "    if save == True:\n",
    "        with open(save_path + '.pkl', 'wb') as fout:\n",
    "            pickle.dump((vectorizer, dtm, lda), fout)\n",
    "        with open(save_path + '_dtm' + '.pkl', 'wb') as fout:\n",
    "            pickle.dump(dtm, fout)\n",
    "        with open(save_path + '_doc_topic_matrix.pkl', 'wb') as fout:\n",
    "            pickle.dump(doc_topic_matrix, fout)\n",
    "    \n",
    "    return(vectorizer, dtm, lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "098bb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_of_whole_date(version = 'v1'):\n",
    "    \"\"\"\n",
    "    train the lda model from 2008 to 2019\n",
    "    \"\"\"\n",
    "    for x in range(2008, 2020):\n",
    "        train_lda_of_year(x, f'./data/lda_model_{x}_{version}', save = True, tf_idf = False, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4256d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment it out as we already process the data\n",
    "# train_lda_of_whole_date('v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "36733840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_topics_to_csv_with_pandas(year, version):\n",
    "    txt_file_path = f'./data/lda_model_{year}_{version}_topics.txt'\n",
    "    topics_data = []\n",
    "\n",
    "    with open(txt_file_path, 'r') as txt_file:\n",
    "        for line in txt_file:\n",
    "            # Extract topic index and keywords\n",
    "            parts = line.strip().split(': ')\n",
    "            topic_index = parts[0].split(' ')[1]  # Assumes format \"Topic X\"\n",
    "            keywords = parts[1] if len(parts) > 1 else ''\n",
    "\n",
    "            # Append to data list\n",
    "            topics_data.append({\n",
    "                'Year': year,\n",
    "                'Topic Index': topic_index,\n",
    "                'Keywords': keywords,\n",
    "                'Topic Category': '',  # Empty for now\n",
    "                'Topic Story': ''      # Empty for now\n",
    "            })\n",
    "\n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(topics_data)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3986b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_topics(data_path, version = 'v1'):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    yearly_topic_data = []\n",
    "    for y in range(2008, 2020):\n",
    "        yearly_topic_data.append(convert_topics_to_csv_with_pandas(y, version))\n",
    "    \n",
    "    df = yearly_topic_data[0]\n",
    "    for i in range(1, len(yearly_topic_data)):\n",
    "        df = pd.concat([df, yearly_topic_data[i]], ignore_index=True)\n",
    "    \n",
    "    df.to_csv(data_path, index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "a9ef59f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_all_topics('./data/topics_iphonve_v2.csv', 'v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e551b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
