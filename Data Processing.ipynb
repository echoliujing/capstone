{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6970e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714822f",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "### Define the data structure of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f440ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_gold - true/false - If true, will only match if the author has reddit gold. If false, will only match if they do not have gold.\n",
    "# is_submitter - true/false - (only relevant when checking comments) If true, will only match if the author \n",
    "# was also the submitter of the post being commented inside. If false, will only match if they were not.\n",
    "# send_replies â€“ When True, messages will be sent to the submission author when comments are made to the submission\n",
    "\n",
    "data_type = {\"subreddit\": \"string\", \"subreddit_id\": \"string\", \"subreddit_type\": \"string\", \"author\": \"string\", \"body\" : \"string\", \n",
    "            \"created_date\" : \"string\", \"created_utc\": \"string\", \"retrieved_on\" : \"string\", \n",
    "            \"id\": \"string\", \"parent_id\": \"string\", \"link_id\": \"string\", \"score\": \"int\", \"total_awards_received\": \"int\", \n",
    "            \"controversiality\": \"int\", \"gilded\": \"int\", \n",
    "            \"collapsed_because_crowd_control\": \"int\", \"collapsed_reason\": \"string\", \"distinguished\": \"string\", \"removal_reason\": \"string\",\n",
    "            \"author_created_utc\": \"string\", \"author_fullname\": \"string\", \"author_patreon_flair\": \"bool\", \"author_premium\": \"bool\",\n",
    "            \"can_gild\": \"bool\", \"can_mod_post\": \"bool\", \"collapsed\": \"bool\", \"is_submitter\": \"bool\", \"_edited\": \"string\", \"locked\": \"bool\",\n",
    "            \"quarantined\": \"bool\", \"no_follow\": \"bool\", \"send_replies\": \"bool\", \"stickied\": \"bool\", \"author_flair_text\": \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f883420",
   "metadata": {},
   "source": [
    "### read all the csv files using the defined data structure, remove deleted rows\n",
    "\n",
    "Some of the rows are marked as \"deleted\" which indicate the contents had been removed for some reasons. We need to remove then as they do not contribute to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd60c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x in range(2008, 2020):\\n    df = pd.read_csv(f'./data/{x}.csv')\\n    df = df[(df.body != '[deleted]') & (df.body != '[removed]')]\\n    df.drop(columns = ['Unnamed: 0'], inplace = True)\\n    df.to_csv(f'./data/{x}_revised.csv')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data and remove [deleted] row\n",
    "# we comment out the codes as we already process the data\n",
    "\n",
    "\"\"\"\n",
    "for x in range(2008, 2020):\n",
    "    df = pd.read_csv(f'./data/{x}.csv')\n",
    "    df = df[(df.body != '[deleted]') & (df.body != '[removed]')]\n",
    "    df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    df.to_csv(f'./data/{x}_revised.csv')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc409f27",
   "metadata": {},
   "source": [
    "### Identify All Posts Related to iPhone on Reddit\n",
    "\n",
    "We conducted a review and found that the subreddits **apple** and **iphone** contain the most relevant discussions about the iPhone. However, the 'apple' subreddit also includes posts about other Apple products like Macs, iPod Touch, and iTunes. To ensure relevance, we will filter out these non-iPhone related posts. Our approach assumes that all posts within a single discussion thread focus on the same topic. Therefore, we will retain any thread in the 'apple' subreddit if at least one post within that thread mentions the iPhone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0cbee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_iphone_post(data):\n",
    "    \"\"\"\n",
    "    Find the parent_ids that contain discussions about iPhone.\n",
    "    \"\"\"\n",
    "    df_apple = data[data.subreddit == 'apple']\n",
    "    related_discussion = df_apple[df_apple.body.str.contains('iphone', flags = re.IGNORECASE)]\n",
    "    return(related_discussion.parent_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac180e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_iphone_post(data):\n",
    "    ids = locate_iphone_post(data)\n",
    "    return(data[(data.subreddit == 'iphone') | (data.parent_id.isin(ids))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfe82ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final clean, only retain iPhone related posts each year\n",
    "\n",
    "def get_iphone_data_yearly(data, filename = None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = extract_all_iphone_post(data)\n",
    "    df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "    if filename != None:\n",
    "        df.to_csv(filename)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd7ef2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor x in range(2008, 2020):\\n    file_name = f'./data/{x}_revised.csv'\\n    save_path = f'./data/{x}_iphone.csv'\\n    data = pd.read_csv(file_name, dtype=data_type, header = 0)\\n    get_iphone_data_yearly(data, filename = save_path)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment the following code because we already run it\n",
    "\"\"\"\n",
    "for x in range(2008, 2020):\n",
    "    file_name = f'./data/{x}_revised.csv'\n",
    "    save_path = f'./data/{x}_iphone.csv'\n",
    "    data = pd.read_csv(file_name, dtype=data_type, header = 0)\n",
    "    get_iphone_data_yearly(data, filename = save_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d84e32",
   "metadata": {},
   "source": [
    "### process the special chars in the comment\n",
    "\n",
    "It seems that the Reddit comment data are raw text data which means the HTML character entities are preserved. For example, we ofter encounter the following chars in the comment:\n",
    "\n",
    "1. `&lt;` is an HTML entity for the less-than symbol (\"<\").\n",
    "2. `&gt;` is an HTML entity for the greater-than symbol (\">\").\n",
    "\n",
    "We need to convert these entities back to their original characters to ensure that the text is correctly interpreted and analyzed. This can usually be done using HTML parsing libraries.\n",
    "\n",
    "Also, escape sequences such as `\\n` and `\\'` need to be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8f5184fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_escape_seq(data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    while data.body.str.contains('&gt;').sum() > 0:\n",
    "        data['body'] = data['body'].apply(html.unescape)\n",
    "        \n",
    "    # Replace common escape sequences\n",
    "    escape_sequences = {\"\\\\n\": \" \", \"\\\\r\": \" \", \"\\\\t\": \" \", \"\\\\'\": \"'\", '\\\\\"': '\"', '\\\\b': '', '\\\\0': ''}\n",
    "    for seq, replacement in escape_sequences.items():\n",
    "        data['body'] = data['body'].str.replace(seq, replacement, regex = False)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0623f3c",
   "metadata": {},
   "source": [
    "LDA\n",
    "\n",
    "to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1e170a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Topic 0:': 'amp, network, like, service, contract, 3g, battery, year, make, know',\n",
       " 'Topic 1:': 'like, keyboard, want, apps, screen, sm, feature, time, button, make',\n",
       " 'Topic 2:': 'case, screen, battery, new, contact, like, time, 3g, store, replace',\n",
       " 'Topic 3:': 'update, post, link, yes, new, reddit, hope, mm, sorry, paste',\n",
       " 'Topic 4:': 'app, like, apps, developer, comment, reddit, store, gt, make, time',\n",
       " 'Topic 5:': 'itunes, jailbreak, restore, firmware, jailbreaking, 3gs, jailbroken, file, dev, tool',\n",
       " 'Topic 6:': 'memory, apps, gt, app, problem, safari, running, jailbroken, run, time',\n",
       " 'Topic 7:': 'app, apps, 3g, setting, icon, theme, sbsettings, cydia, like, thing',\n",
       " 'Topic 8:': 'game, app, google, free, like, great, version, 99, fun, play',\n",
       " 'Topic 9:': 'know, game, time, like, sure, want, good, 10, way, look',\n",
       " 'Topic 10:': 'plan, data, user, month, photo, carrier, pay, voice, free, year',\n",
       " 'Topic 11:': 'app, apps, flash, page, store, video, want, site, web, like',\n",
       " 'Topic 12:': 'work, code, used, like, 3g, got, pretty, time, bought, touch',\n",
       " 'Topic 13:': 'sound, problem, bluetooth, love, headphone, check, like, ear, work, right',\n",
       " 'Topic 14:': 'game, like, look, word, turn, button, good, make, thing, screen'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.read_csv('./data/2009.csv', dtype=data_type, header = 0)\n",
    "df_trial = df_2\n",
    "apple_related_comment = df_trial[df_trial.subreddit.isin(['iphone']) & (df_trial.body != '[deleted]')].loc[:, ('parent_id', 'body')]\n",
    "\n",
    "# Grouping comments by 'parent_id'\n",
    "apple_related_comment = apple_related_comment.groupby('parent_id')['body'].apply(' '.join).reset_index()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('iphone')\n",
    "stopwords.append('use')\n",
    "stopwords.append('people')\n",
    "stopwords.append('http')\n",
    "stopwords.append('www')\n",
    "stopwords.append('com')\n",
    "stopwords.append('really')\n",
    "stopwords.append('apple')\n",
    "stopwords.append('actually')\n",
    "stopwords.append('thanks')\n",
    "stopwords.append('thank')\n",
    "stopwords.append('think')\n",
    "stopwords.append('phone')\n",
    "stopwords.append('oh')\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z0-9]', ' ', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Removing stopwords and lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    # Rejoining the words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# Cleaning the comments\n",
    "apple_related_comment['cleaned_body'] = apple_related_comment['body'].apply(clean_text)\n",
    "\n",
    "# Displaying the first few rows of cleaned data\n",
    "apple_related_comment[['body', 'cleaned_body']].head()\n",
    "\n",
    "# Extracting features for LDA\n",
    "vectorizer = CountVectorizer(max_df = 0.8, min_df = 10, stop_words = 'english')\n",
    "dtm = vectorizer.fit_transform(apple_related_comment['cleaned_body'])\n",
    "\n",
    "# Fitting LDA model\n",
    "lda = LatentDirichletAllocation(n_components=15, random_state=0)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d:\" % (topic_idx)] = \", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    return topic_dict\n",
    "\n",
    "# Displaying the topics\n",
    "no_top_words = 10\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7ba73cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('./data/2009.csv', dtype=data_type, header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5c5343b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial = df_2\n",
    "apple_related_comment = df_trial[df_trial.subreddit.isin(['iphone']) & (df_trial.body != '[deleted]')].loc[:, ('parent_id', 'body')]\n",
    "\n",
    "# Grouping comments by 'parent_id'\n",
    "apple_related_comment = apple_related_comment.groupby('parent_id')['body'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "86a8033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kalok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading necessary NLTK components\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('iphone')\n",
    "stopwords.append('use')\n",
    "stopwords.append('people')\n",
    "stopwords.append('http')\n",
    "stopwords.append('www')\n",
    "stopwords.append('com')\n",
    "stopwords.append('really')\n",
    "stopwords.append('apple')\n",
    "stopwords.append('actually')\n",
    "stopwords.append('thanks')\n",
    "stopwords.append('thank')\n",
    "stopwords.append('think')\n",
    "stopwords.append('phone')\n",
    "stopwords.append('oh')\n",
    "stopwords.append('sorry')\n",
    "stopwords.append('hi')\n",
    "stopwords.append('even')\n",
    "\n",
    "tech_term = [ 'app', 'apps', 'iphone', 'phone', 'device', 'mobile', 'screen', 'button', 'battery', 'camera', 'keyboard', \n",
    "               'wifi', 'network', 'service', 'call', 'data', 'version', 'update', 'store', 'itunes', 'cydia', 'firmware', 'jailbreak', 'jailbroken', \n",
    "               'jailbreaking', 'flash', 'link', 'post', 'code', 'file', 'download', 'sync', 'contact', 'email', 'gps', '3g', '3gs', 'icon', 'theme', \n",
    "               'sbsettings', 'winterboard', 'blog', 'dev', 'tool', 'setting', 'settings', 'card', 'memory', 'photo', 'video', 'youtube', 'promo', \n",
    "               'promo code', 'gt', 'org', 'pdf', 'mp3', 'ipod', 'itunes', 'mac', 'apple', 'safari', 'google', 'gmail', 'stanza', 'verizon', \n",
    "               'customer', 'contract', 'price', 'unlock', 'install', 'price', 'free', 'developer', 'web', 'site', 'page', 'application', 'app store', 'view', \n",
    "               'look like', 'touch']\n",
    "\n",
    "common_term = ['like', 'one', 'get', 'would', 'know', 'think', 'make', 'go', 'want', 'need', 'say', 'come', 'time', 'take', 'see', 'look', 'use', 'really', \n",
    "               'good', 'great', 'well', 'still', 'also', 'much', 'could', 'way', 'thing', 'got', 'around', 'first', 'new', 'lot', 'try', 'might', 'even', \n",
    "               'something', 'anything', 'everything', 'nothing', 'day', 'year', 'month', 'week', 'today', 'yesterday', 'tomorrow', 'now', 'then', 'here', \n",
    "               'there', 'where', 'why', 'how', 'what', 'which', 'who', 'whom', 'amp', 'let', 'put', 'end', 'start', 'seem', 'feel', 'sound', 'look', 'tend', \n",
    "               'may', 'might', 'must', 'will', 'shall', 'can', 'could', 'should', 'would', 'did', 'do', 'does', 'done', 'have', 'has', 'had', 'give', 'given', \n",
    "               'gave', 'take', 'taken', 'took', 'say', 'said', 'telling', 'tell', 'told', 'go', 'went', 'gone', 'going', 'keep', 'kept', 'keeping', 'seem', \n",
    "               'seemed', 'seeming', 'seems', 'become', 'became', 'becomes', 'becoming', 'stay', 'stayed', 'staying', 'stays', 'fall', 'fell', 'fallen', 'falling', \n",
    "               'stand', 'stood', 'standing', 'stands', 'become', 'became', 'becoming', 'becomes', 'come', 'came', 'coming', 'comes', 'provide', 'provided', 'provides', \n",
    "               'providing', 'include', 'included', 'includes', 'including', 'continue', 'continued', 'continues', 'continuing', 'expect', 'expected', 'expecting', \n",
    "               'expects', 'hope', 'hoped', 'hopes', 'hoping', 'appear', 'appeared', 'appearing', 'appears', 'remain', 'remained', 'remaining', 'remains', 'suggest', \n",
    "               'suggested', 'suggesting', 'suggests', 'want', 'wanted', 'wanting', 'wants', 'wish', 'wished', 'wishes', 'wishing', 'prefer', 'preferred', 'preferring', \n",
    "               'prefers', 'desire', 'desired', 'desires', 'desiring', 'love', 'loved', 'loves', 'loving', 'like', 'liked', 'likes', 'liking', 'admire', 'admired', \n",
    "               'admires', 'admiring', 'appreciate', 'appreciated', 'appreciates', 'appreciating', 'value', 'valued', 'values', 'valuing', 'choose', 'chose', 'chosen']\n",
    "\n",
    "#stopwords.extend(tech_term)\n",
    "#stopwords.extend(common_term)\n",
    "\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z0-9%]', ' ', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Removing stopwords and lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords]\n",
    "    # Rejoining the words back into a single string\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d5710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "be3bef87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>cleaned_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That would never fit into any normal pocket.  ...</td>\n",
       "      <td>would never fit normal pocket ipod big stretch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You apparently got downvoted for criticizing A...</td>\n",
       "      <td>apparently got downvoted criticizing lame prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+1 for you. Relax. One person downvoted him, h...</td>\n",
       "      <td>1 relax one person downvoted 4 big conspiracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is a big conspiracy. It's reddit.</td>\n",
       "      <td>big conspiracy reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WORST comes to worst you do a restore and you ...</td>\n",
       "      <td>worst come worst restore working nothing lose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  That would never fit into any normal pocket.  ...   \n",
       "1  You apparently got downvoted for criticizing A...   \n",
       "2  +1 for you. Relax. One person downvoted him, h...   \n",
       "3               It is a big conspiracy. It's reddit.   \n",
       "4  WORST comes to worst you do a restore and you ...   \n",
       "\n",
       "                                        cleaned_body  \n",
       "0  would never fit normal pocket ipod big stretch...  \n",
       "1  apparently got downvoted criticizing lame prod...  \n",
       "2      1 relax one person downvoted 4 big conspiracy  \n",
       "3                              big conspiracy reddit  \n",
       "4      worst come worst restore working nothing lose  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the comments\n",
    "apple_related_comment['cleaned_body'] = apple_related_comment['body'].apply(clean_text)\n",
    "\n",
    "# Displaying the first few rows of cleaned data\n",
    "apple_related_comment[['body', 'cleaned_body']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "deb0207e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('app store', 219),\n",
       "  ('3 0', 199),\n",
       "  ('3 1', 134),\n",
       "  ('look like', 81),\n",
       "  ('ipod touch', 76),\n",
       "  ('sound like', 74),\n",
       "  ('1 2', 72),\n",
       "  ('battery life', 57),\n",
       "  ('pretty much', 56),\n",
       "  ('seems like', 54)],\n",
       " [('3 1 2', 52),\n",
       "  ('webobjects mzstore woa', 27),\n",
       "  ('mzstore woa wa', 27),\n",
       "  ('youtube watch v', 26),\n",
       "  ('woa wa viewsoftware', 26),\n",
       "  ('wa viewsoftware id', 26),\n",
       "  ('amp mt 8', 26),\n",
       "  ('itunes webobjects mzstore', 25),\n",
       "  ('en wikipedia org', 23),\n",
       "  ('wikipedia org wiki', 23)])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract n-grams (bigrams and trigrams)\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(data.split(), num)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# Extracting bigrams and trigrams\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "for doc in apple_related_comment['cleaned_body']:\n",
    "    bigrams.extend(extract_ngrams(doc, 2))\n",
    "    trigrams.extend(extract_ngrams(doc, 3))\n",
    "\n",
    "# Counting occurrences of bigrams and trigrams\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# Displaying the most common bigrams and trigrams\n",
    "most_common_bigrams = bigram_counts.most_common(10)\n",
    "most_common_trigrams = trigram_counts.most_common(10)\n",
    "\n",
    "most_common_bigrams, most_common_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fc4a22cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topic 0:': 'amp, 3g, call, network, data, service, would, year, contract, plan, month, 3gs, make, get, like',\n",
       " 'Topic 1:': 'game, play, app, work, good, free, get, fun, great, version, like, music, control, time, playing',\n",
       " 'Topic 2:': 'google, gt, contact, sync, like, know, yes, sure, would, work, one, calendar, way, photo, voice',\n",
       " 'Topic 3:': 'jailbreak, apps, jailbroken, jailbreaking, cydia, get, itunes, restore, firmware, need, install, app, device, store, 3gs',\n",
       " 'Topic 4:': 'case, like, screen, button, make, one, keyboard, would, also, look, feel, type, find, great, time',\n",
       " 'Topic 5:': 'app, store, app store, apps, get, developer, would, time, pretty, buy, good, make, money, like, one',\n",
       " 'Topic 6:': 'get, see, new, update, got, one, like, mm, used, first, could, please, time, would, guy',\n",
       " 'Topic 7:': 'apps, app, battery, icon, application, push, theme, background, sbsettings, text, cydia, life, much, like, mail',\n",
       " 'Topic 8:': 'reddit, comment, app, flash, get, way, back, page, like, video, story, see, would, right, link',\n",
       " 'Topic 9:': 'new, would, like, make, release, org, much, blog, working, probably, know, one, something, bug, never',\n",
       " 'Topic 10:': 'code, touch, ipod, first, work, ipod touch, promo code, got, promo, view, take, also, screen, app, look',\n",
       " 'Topic 11:': 'like, memory, look, game, free, one, apps, well, seems, song, would, try, yeah, update, tried'}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting features for LDA\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), max_df = 0.8, min_df = 25)\n",
    "dtm = vectorizer.fit_transform(apple_related_comment['cleaned_body'])\n",
    "\n",
    "# Fitting LDA model\n",
    "lda = LatentDirichletAllocation(n_components=12, random_state=0)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Function to display topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d:\" % (topic_idx)] = \", \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    return topic_dict\n",
    "\n",
    "# Displaying the topics\n",
    "no_top_words = 15\n",
    "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c16a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "bd632c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623.6987584402699"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "75672367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-512819.72850376583"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.score(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "95493f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 8.51%\n",
      "Topic 1: 8.05%\n",
      "Topic 2: 7.41%\n",
      "Topic 3: 7.71%\n",
      "Topic 4: 9.43%\n",
      "Topic 5: 10.94%\n",
      "Topic 6: 9.26%\n",
      "Topic 7: 7.12%\n",
      "Topic 8: 8.88%\n",
      "Topic 9: 7.96%\n",
      "Topic 10: 6.38%\n",
      "Topic 11: 8.35%\n"
     ]
    }
   ],
   "source": [
    "# Get topic distribution for each document\n",
    "topic_distributions = lda.transform(dtm)\n",
    "\n",
    "# Calculate the weight of each topic\n",
    "topic_weights = topic_distributions.sum(axis=0) / topic_distributions.sum()\n",
    "\n",
    "# Print the weight of each topic\n",
    "for topic_idx, topic_weight in enumerate(topic_weights):\n",
    "    print(f\"Topic {topic_idx}: {topic_weight:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7b79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee519041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7dfe5679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56647282, 0.00666667, 0.00666667, ..., 0.0066667 , 0.00666667,\n",
       "        0.00666668],\n",
       "       [0.00512821, 0.00512821, 0.00512821, ..., 0.00512821, 0.00512822,\n",
       "        0.00512822],\n",
       "       [0.01666677, 0.01666671, 0.01666669, ..., 0.01666674, 0.01666669,\n",
       "        0.76666588],\n",
       "       ...,\n",
       "       [0.00606063, 0.00606062, 0.00606061, ..., 0.00606062, 0.00606062,\n",
       "        0.00606062],\n",
       "       [0.00333334, 0.00333334, 0.00333333, ..., 0.00333334, 0.00333334,\n",
       "        0.00333335],\n",
       "       [0.02222227, 0.02222225, 0.02222227, ..., 0.02222223, 0.02222222,\n",
       "        0.02222225]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0c321a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.007*\"rice\" + 0.003*\"bitching\" + 0.003*\"toilet\" + 0.003*\"free app\" + 0.003*\"parallel\" + 0.003*\"phone really\" + 0.003*\"rotated\" + 0.002*\"nerd\" + 0.002*\"launching app\" + 0.002*\"hate spotlight\"'),\n",
       " (1,\n",
       "  '0.005*\"air\" + 0.004*\"apt\" + 0.004*\"regarding\" + 0.003*\"many people\" + 0.003*\"dark\" + 0.003*\"23\" + 0.002*\"upvote\" + 0.002*\"geohot\" + 0.002*\"get work\" + 0.001*\"furthermore\"'),\n",
       " (2,\n",
       "  '0.003*\"dollar\" + 0.003*\"white\" + 0.003*\"art\" + 0.003*\"lie\" + 0.002*\"nsfw\" + 0.002*\"attempt\" + 0.002*\"gadget\" + 0.002*\"find one\" + 0.002*\"operation\" + 0.002*\"would great\"'),\n",
       " (3,\n",
       "  '0.002*\"protection\" + 0.002*\"know work\" + 0.002*\"paragraph\" + 0.002*\"apps without\" + 0.001*\"advertisement\" + 0.001*\"download apps\" + 0.001*\"bigboss\" + 0.001*\"radio station\" + 0.001*\"know saurik\" + 0.001*\"via itunes\"'),\n",
       " (4,\n",
       "  '0.020*\"iphone\" + 0.013*\"phone\" + 0.012*\"app\" + 0.009*\"like\" + 0.009*\"one\" + 0.008*\"would\" + 0.008*\"get\" + 0.008*\"use\" + 0.008*\"apple\" + 0.007*\"time\"'),\n",
       " (5,\n",
       "  '0.006*\"rock\" + 0.005*\"stanza\" + 0.005*\"http\" + 0.004*\"iphone\" + 0.004*\"june\" + 0.004*\"12\" + 0.003*\"area\" + 0.003*\"package\" + 0.003*\"18\" + 0.003*\"50\"'),\n",
       " (6,\n",
       "  '0.005*\"lying\" + 0.004*\"hundred\" + 0.003*\"manual\" + 0.002*\"improving\" + 0.002*\"people know\" + 0.002*\"right side\" + 0.002*\"jump\" + 0.002*\"fuck\" + 0.002*\"square\" + 0.002*\"springboard\"'),\n",
       " (7,\n",
       "  '0.006*\"ifixit\" + 0.005*\"water\" + 0.004*\"ifixit com\" + 0.003*\"repair\" + 0.003*\"defense\" + 0.003*\"dry\" + 0.003*\"thing happened\" + 0.003*\"next iphone\" + 0.003*\"took apple\" + 0.002*\"damage\"'),\n",
       " (8,\n",
       "  '0.004*\"upload\" + 0.003*\"thanks code\" + 0.003*\"might worth\" + 0.002*\"low memory\" + 0.002*\"pointless\" + 0.002*\"use case\" + 0.002*\"reinstalling\" + 0.001*\"phrase\" + 0.001*\"deleting\" + 0.001*\"german\"'),\n",
       " (9,\n",
       "  '0.015*\"game\" + 0.012*\"http\" + 0.011*\"com\" + 0.011*\"look\" + 0.010*\"thanks\" + 0.009*\"www\" + 0.009*\"http www\" + 0.007*\"code\" + 0.005*\"love\" + 0.005*\"file\"'),\n",
       " (10,\n",
       "  '0.006*\"google phone\" + 0.003*\"fellow\" + 0.003*\"greeting\" + 0.003*\"confusing\" + 0.002*\"would great\" + 0.002*\"racing\" + 0.002*\"make iphone\" + 0.001*\"freaking\" + 0.001*\"incoming\" + 0.001*\"get rid\"'),\n",
       " (11,\n",
       "  '0.004*\"macrumors\" + 0.003*\"macrumors com\" + 0.002*\"burned\" + 0.002*\"dollar\" + 0.002*\"forever\" + 0.002*\"megapixel camera\" + 0.002*\"enjoy\" + 0.002*\"vote\" + 0.002*\"steve job\" + 0.001*\"mozilla\"'),\n",
       " (12,\n",
       "  '0.003*\"graph\" + 0.002*\"spot\" + 0.002*\"lab\" + 0.002*\"define\" + 0.001*\"half assed\" + 0.001*\"found way\" + 0.001*\"replicate\" + 0.001*\"people start\" + 0.001*\"waste time\" + 0.001*\"store yet\"'),\n",
       " (13,\n",
       "  '0.003*\"moron\" + 0.003*\"pie\" + 0.002*\"mouse\" + 0.002*\"tilt\" + 0.002*\"anywhere near\" + 0.002*\"site http\" + 0.001*\"scoring\" + 0.001*\"app see\" + 0.001*\"photoshop\" + 0.001*\"page look\"'),\n",
       " (14,\n",
       "  '0.004*\"cdma\" + 0.003*\"offered\" + 0.003*\"bright\" + 0.003*\"shop\" + 0.003*\"apple com\" + 0.003*\"http itunes\" + 0.002*\"cake\" + 0.002*\"id\" + 0.002*\"make one\" + 0.002*\"january\"')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# Creating bigrams and trigrams\n",
    "bigram = Phrases(apple_related_comment['cleaned_body'], min_count=5, threshold=100)\n",
    "trigram = Phrases(apple_related_comment['cleaned_body'], threshold=100)\n",
    "\n",
    "# Phraser for efficiency\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "# Function to form bigrams and trigrams\n",
    "def make_ngrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# Applying the function to our dataset\n",
    "ngrams_corpus = make_ngrams(apple_related_comment['cleaned_body'])\n",
    "\n",
    "# Creating a new dictionary and corpus for LDA with n-grams\n",
    "dictionary_ngrams = corpora.Dictionary(ngrams_corpus)\n",
    "dictionary_ngrams.filter_extremes(no_below=20, no_above=0.85)\n",
    "corpus_ngrams = [dictionary_ngrams.doc2bow(text) for text in ngrams_corpus]\n",
    "\n",
    "# Applying LDA using Gensim on the data with n-grams\n",
    "lda_model_with_ngrams = LdaModel(corpus_with_ngrams, num_topics=15, id2word=dictionary_with_ngrams, passes=15)\n",
    "\n",
    "# Extracting and displaying topics with n-grams\n",
    "lda_topics_with_ngrams = lda_model_with_ngrams.print_topics(num_words=10)\n",
    "lda_topics_with_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d3059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
